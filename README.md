# 项目介绍 

用pytorch进行CIFAR-10数据集分类

## 数据集介绍

​		该数据集共有60000张彩色图像，这些图像是32*32，分为10个类，每类6000张图。这里面有50000张用于训练，构成了5个训练批，每一批10000张图；另外10000用于测试，单独构成一批。测试批的数据里，取自10类中的每一类，每一类随机取1000张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有5000张图。

下面这幅图就是列举了10各类，每一类展示了随机的10张图片：

![10](C:\Users\19367\Desktop\作业\人工智能安全\README.assets\10.png)

数据集的下载路径：http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

## 代码逻辑

1. 使用torchvision加载并预处理CIFAR-10数据集、
2. 定义网络 
3. 定义损失函数和优化器 
4. 训练网络并更新网络参数 
5. 测试网络

## Requirement

- Python 3.6+
- torch1.0
- numpy
- torchvision
- matplotlib

## 超参数实现

​		本文超参数选择了学习率作为研究对象，分别应用0.0005、0.001、0.0015三种学习率进行实验，并将其绘制成折线图，探究其对train过程中loss的影响，由于时间原因，实验采用的epoch数为20，基本已经可以得到实验结论。

## 运行结果
#### learning rate = 0.0005：

<img src="C:\Users\19367\Desktop\作业\人工智能安全\README.assets\image-20210924210725631.png" alt="image-20210924210725631" style="zoom: 80%;" />

#### learning rate = 0.001:

<img src="C:\Users\19367\Desktop\作业\人工智能安全\README.assets\image-20210924210742710.png" alt="image-20210924210742710" style="zoom:80%;" />

#### learning rate = 0.0015:

<img src="C:\Users\19367\Desktop\作业\人工智能安全\README.assets\image-20210924210829761.png" alt="image-20210924210829761" style="zoom:80%;" />

#### 不同学习率下的train_loss曲线

<img src="C:\Users\19367\Desktop\作业\人工智能安全\README.assets\image-20210925154105542.png" alt="image-20210925154105542" style="zoom:67%;" />

## 结果分析

​		从不同学习率下的train_loss曲线图像可以看出，学习率最大的绿色曲线在一开始收敛速度是最快的，而学习率最小的蓝色曲线收敛的速度最慢。但是随着epoch数的增加，学习率最大的绿色曲线很快在loss较大的地方收敛了，学习率最小的曲线在loss较小的地方才刚开始收敛，最终导致学习率大的准确率较低，而学习率小的准确率更高。

​		和我们课上所学的知识一致：学习率太大会很难逼近最优值，但是其收敛速度是很快的，如果离最优点较远，其可以更快的逼近最优点。最好的方法是引入动态学习率，在一开始采用较大的学习率快速逼近loss函数的最低值，然后在最低值附近变成较小的学习率，防止错过波谷，更好的逼近最优值，这样在效率和质量上都可以达到比较好的效果。



